# Knowledge Distillation for Molecular Property Prediction: A Scalability Analysis

This repository provides a framework for training teacher and student models for molecular property prediction using **Knowledge Distillation (KD)**. The implementation supports training on the **QM9, ESOL, and FreeSolv** datasets and utilizes **graph neural networks (GNNs)** like **SchNet, DimeNet++, and TensorNet**.

<p align="center">
  <img width="396" alt="image" src="[https://github.com/user-attachments/assets/c08ad6dc-22bf-4f2f-bfce-b1d43172b6ce](https://github.com/PEESEgroup/Knowledge-Distillation-For-Molecular-Properties/blob/main/TOC.png)" />
</p>

## ğŸ“Œ Features
- Train **teacher models** on QM9 (first 5 targets).
- Train **student models** with or without **knowledge distillation** on 10 different QM9 and two experimental datasets within MoleculeNet (ESOL/FreeSolv).
- Supports **SchNet, DimeNet++, and TensorNet** models.
- Implements **Uncertainity-weighted ensemble of L1 loss and Cosine Similarity loss** for our regression-based KD approach.
- **Hyperparameter tuning** using **Optuna**.
- **Early stopping & model checkpointing**.
- **Logging** of metrics and best models.

## ğŸ“‚ Repository Structure
```
Knowledge_distillation_for_molecules/
â”‚ 
â”‚â”€â”€ architectures/               # Model architectures
â”‚   â”œâ”€â”€ schnet.py                # SchNet architectures
â”‚   â”œâ”€â”€ dimenetpp.py             # DimeNet++ architectures
â”‚   â”œâ”€â”€ tensornet.py             # TensorNet architectures
â”‚â”€â”€ training/                    # Training scripts
â”‚   â”œâ”€â”€ train_teacher.py         # Train teacher models
â”‚   â”œâ”€â”€ train_student.py         # Train student models (with/without KD)
â”‚   â”œâ”€â”€ total_loss.py            # Loss function (L1 + Cosine similarity)
â”‚   â”œâ”€â”€ optimize_hyperparams.py  # Optuna-based hyperparameter tuning  âœ…
â”œâ”€â”€ data/                        # Data processing
â”‚   â”œâ”€â”€ data_loader.py           # Loads QM9, ESOL, FreeSolv datasets
â”œâ”€â”€ utils/                       # Helper functions
â”‚   â”œâ”€â”€ train_utils.py           # Metrics, reproducibility
â”‚   â”œâ”€â”€ config.py                # Centralized hyperparameters
â”‚â”€â”€ results/                          # Logs, saved models (will be created during model checkpoint saving)
â”‚â”€â”€ README.md                         # Documentation
â”‚â”€â”€ requirements.txt                  # Dependencies
```

## ğŸš€ Installation

### **Using mamba for dependency management**
To set up the environment and install dependencies, including `torchmd-net`:

```bash
mamba create -n molecular-learning python=3.9
mamba activate molecular-learning
mamba install pytorch torchvision torchaudio -c pytorch
mamba install pyg -c pyg
mamba install torchmd-net -c conda-forge
pip install -r requirements.txt
```

## ğŸ”¥ Training
### **1ï¸âƒ£ Train the Teacher Model**
Run the following command to train a **teacher model**:
```bash
python training/train_teacher.py
```
This trains a **SchNet/DimeNet++/TensorNet** model on **QM9** (first 5 targets).

### **2ï¸âƒ£ Train the Student Model (With or Without KD)**
To train the **student model**, run:
```bash
python training/train_student.py
```
- **With KD:** The script will use the pre-trained **teacher model**.
- **Without KD:** Set `USE_KD = False` in `config.py`.

## ğŸ” Hyperparameter Tuning
To tune **learning rate, batch size, and alpha** using **Optuna**, run:
```bash
python training/optimize_hyperparams.py
```
- The best hyperparameters will be **saved in `config_optimized.json`**.
- To apply them automatically, ensure `PERFORM_TUNING = False` in `config.py`.

## ğŸ“Š Logging & Results
- **Best models** are saved in `results/`.
- **Metrics (train loss, val loss, RÂ²)** are logged in `config.LOG_PATH`.

## ğŸ“¥ Pretrained Models
- Pretrained teacher and student models for QM9, ESOL, and FreeSolv can be downloaded here - [pretrained models](https://drive.google.com/drive/folders/1k_N6Cswk57DlxprMFuArh-oaxTz_V-xi?usp=sharing)

## ğŸ“œ Citation
- If you find this repository useful, please cite our work:
  
```
@article{molKD2025,
  author    = {Sheshanarayana, Rahul and You, Fengqi},
  title     = {Knowledge Distillation for Molecular Property Prediction: A Scalability Analysis},
  journal   = {Advanced Science},
  volume    = {n/a},
  number    = {n/a},
  pages     = {# insert after published #},
  keywords  = {materials informatics, graph neural networks, scalability, knowledge distillation},
  doi       = {# insert after published #},
  url       = {# insert after published #},
  eprint    = {# insert after published #},
  abstract  = {# insert after published #}
}
```
---

Feel free to contribute or raise issues! ğŸš€
